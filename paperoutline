## Structure of Midterm
- Overview
- Time + Space Complexity: Big O
  - table with Big O
  - psedocode explained
- Empirical Data
  - graphs 
  - Operations count 
  - recursion comparision between languages 
  - large N input
  - speed ratio
- Lang Analysis 
  - comparisions
- Discussions
- Conclusions 
- References 

## Final Outline 
- Introduction/Overview
  - Briefly intro decision Tree algo and significance in machine learning 
  - explain the objective of the report
- Algo Background 
  - History of Decision Tree Algo
  - How they work 
  - Types of Decision Trees
- Theoretical Analysis
  - pseudo code
  - time and space complexity 
  - Correctness?
  - Prunning?
  - Compare with other Algos?
- Empirical Analysis
  - Scalibility with small vs large datasetss, compare preformance
  - include visuals
- Implementation
  - Explain the functions
  - Code snippets
  - Testing 
  - Sample Runs
- Discussion 
  - Discuss empirical results 
  - Limitations 
  - Practical Applications
- Conclusion 
  - Summarize Key findings 
  - possible improvements/ extensions
- References 
  - 2 peer reviewed papers 
  - 5 references overall
- 

## There are three types of decision trees

The first developed decision tree algorithm was the by Ross Quinlan who developed ID3, which stands for "Iterative Dichotomiser 3." This version leverages entropy and information gain as metrics to evaluate data splits.

The later interation of ID3 is the algorithm C4.5 also develiped by Quinlan. It utilizes infomration gain and gain ratios to evaulate the split points with the decision trees. [1]  

The third and most recently developed type of decision tree algorithm is 'classification and regression trees" (known as CART). It was introduced by Leo Breiman and uses Gini impourity to identify the ideal attribute to split the data. Gini impurity measure how often a randomly chosen attribute is misclassified. [1]

## Types of Supervised Learning in ML:

Classification: Output is a catagorical variable (spam vs non spam, yes or no). [2]  

Regression: Where the output is a continuous variable (ex: prediciting prices or stock prices). [2]

Supervised machine learning is an approach in machine learning and artifical intelligence. It involves training a model using labeled data with each input having a correspinding correct output. An anaology would be a teacher guiding through the learning process. [2]

### Advantages and Disadvatages to DT [1]

Advantages: Easy to interpret: The Boolean logic and visual representations of decision trees make them easier to understand and consume. The hierarchical nature of a decision tree also makes it easy to see which attributes are most important, which isn’t always clear with other algorithms, like neural networks.

Little to no data preparation required: Decision trees have a number of characteristics, which make it more flexible than other classifiers. It can handle various data types—i.e. discrete or continuous values, and continuous values can be converted into categorical values through the use of thresholds. Additionally, it can also handle values with missing values, which can be problematic for other classifiers, like Naïve Bayes.

More flexible: Decision trees can be leveraged for both classification and regression tasks, making it more flexible than some other algorithms. It’s also insensitive to underlying relationships between attributes; this means that if two variables are highly correlated, the algorithm will only choose one of the features to split on.

Disadvantages: Prone to overfitting: Complex decision trees tend to overfit and do not generalize well to new data. This scenario can be avoided through the processes of pre-pruning or post-pruning. Pre-pruning halts tree growth when there is insufficient data while post-pruning removes subtrees with inadequate data after tree construction.

High variance estimators: Small variations within data can produce a very different decision tree. Bagging, or the averaging of estimates, can be a method of reducing variance of decision trees. However, this approach is limited as it can lead to highly correlated predictors.

More costly: Given that decision trees take a greedy search approach during construction, they can be more expensive to train compared to other algorithms.


Desicion Trees have preferences for small trees, which is consistent with the priniciple of parsimony in Occam's Razor. "entities should not be multplied beyond nessessity." This implies decision trees should only add complexity if neccessary because the simpliest explanation is often the best. To reduce the complexity, pruning is typically used which is when branches are removed that split features of low importance. A group of decision trees called a random forest is often used to maintain accuracy, but here we will focus on the individual decision tree. [^1]